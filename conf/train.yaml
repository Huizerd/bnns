datamodule:
  dir: data/datasets  # data directory
  batch: 128  # batch size
  num_workers: 4  # number of processes for loading data

trainer:
  gpus: -1  # number of GPUs, -1 == all
  max_epochs: 100  # training epochs
  track_grad_norm: 2  # set to -1 to not track grad norm

model:
  act_fn: relu  # relu/binarize_cancel/dont_binarize/only_clamp
  quant_fn: quantize
  quant_params:
    quant_min: -128
    quant_max: 127
    symmetric: True
  hidden_sizes: [512, 512]
  optimizer: torch.optim.Adam
  lr: 0.001

logging:
  dir: logs
  name: quantize_int8
